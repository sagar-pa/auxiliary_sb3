{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Union, Dict, Any, Optional\n",
    "import torch as th\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class AuxiliaryBufferSamples(NamedTuple):\n",
    "    observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    old_values: th.Tensor\n",
    "    old_log_prob: th.Tensor\n",
    "    advantages: th.Tensor\n",
    "    returns: th.Tensor\n",
    "    infos: List[List[Dict[str, Any]]]\n",
    "\n",
    "class AuxiliaryBuffer(RolloutBuffer):\n",
    "    \"\"\"\n",
    "    Rollout buffer used in on-policy algorithms like A2C/PPO, \n",
    "        with an added tracking of information dictionaries.\n",
    "    It corresponds to ``buffer_size`` transitions collected\n",
    "    using the current policy.\n",
    "    This experience will be discarded after the policy update.\n",
    "    In order to use PPO objective, we also store the current value of each state\n",
    "    and the log probability of each taken action.\n",
    "    The term rollout here refers to the model-free notion and should not\n",
    "    be used with the concept of rollout used in model-based RL or planning.\n",
    "    Hence, it is only involved in policy and value function training but not action selection.\n",
    "    :param buffer_size: Max number of element in the buffer\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param device: PyTorch device\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param gamma: Discount factor\n",
    "    :param n_envs: Number of parallel environments\n",
    "    \"\"\"\n",
    "    def __init__(self,         \n",
    "            buffer_size: int,\n",
    "            observation_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            gae_lambda: float = 1,\n",
    "            gamma: float = 0.99,\n",
    "            n_envs: int = 1):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, \n",
    "            gae_lambda=gae_lambda, gamma=gamma, n_envs=n_envs)\n",
    "        \n",
    "        lengths = [buffer_size] * n_envs\n",
    "        self.info_bins = np.cumsum(np.array(lengths))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.infos = [[None] * self.buffer_size for __ in range(self.n_envs)]\n",
    "        super().reset()\n",
    "\n",
    "    def add(self,\n",
    "            obs: np.ndarray,\n",
    "            action: np.ndarray,\n",
    "            reward: np.ndarray,\n",
    "            episode_start: np.ndarray,\n",
    "            value: th.Tensor,\n",
    "            log_prob: th.Tensor,\n",
    "            infos: List[Dict[str, Any]], \n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        :param obs: Observation\n",
    "        :param action: Action\n",
    "        :param reward:\n",
    "        :param episode_start: Start of episode signal.\n",
    "        :param value: estimated value of the current state\n",
    "            following the current policy.\n",
    "        :param log_prob: log probability of the action\n",
    "            following the current policy.\n",
    "        :param infos: the information dictionaries returned by the environment\n",
    "        \"\"\"\n",
    "        for i, info in enumerate(infos):\n",
    "            self.infos[i][self.pos] = deepcopy(info)\n",
    "        super().add(obs, action, reward, episode_start, value, log_prob)\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> AuxiliaryBufferSamples:\n",
    "        rollout_samples = super()._get_samples(batch_inds, env)\n",
    "        infos = []\n",
    "        env_indices = np.digitize(batch_inds, bins=self.info_bins, right=False)\n",
    "        last_env_indices = np.clip(env_indices - 1, 0, self.info_bins.shape[0])\n",
    "        step_indices = batch_inds - self.info_bins[last_env_indices]\n",
    "        for ep_idx, step_idx in zip(env_indices, step_indices):\n",
    "            infos.append(self.infos[ep_idx][step_idx:])\n",
    "        return AuxiliaryBufferSamples(*rollout_samples, infos=infos)\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = spaces.Box(low=np.zeros((3,), dtype=np.float64), high = np.ones((3,), dtype=np.float64), dtype=np.float64)\n",
    "action_space = spaces.Box(low=np.zeros(1, dtype=np.float64), high=np.ones(1, dtype=np.float64), dtype=np.float64)\n",
    "\n",
    "buffer = AuxiliaryBuffer(buffer_size= 10, observation_space=obs_space, action_space = action_space, device=\"cpu\", n_envs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_env = 3\n",
    "for i in range(10):\n",
    "    obs = np.random.randn(n_env, 3,)\n",
    "    act = np.random.randn(n_env, 1,)\n",
    "    reward = np.random.randn(n_env, )\n",
    "    dones = np.random.randn(n_env, ) > 0.5\n",
    "    value = th.randn(n_env, )\n",
    "    log_prob = th.randn(n_env, )\n",
    "    infos = [{\"i\": i, \"j\": j} for j in range(n_env)]\n",
    "    buffer.add(obs, act, reward, dones, value, log_prob, infos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = buffer.get(batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in batch:\n",
    "    print(data.infos)\n",
    "buffer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import torch as th\n",
    "from abc import ABC\n",
    "\n",
    "class AuxiliaryObjective(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def calculate_loss(self, \n",
    "            obs: th.Tensor,\n",
    "            features: th.Tensor,\n",
    "            actions: th.Tensor,\n",
    "            returns: th.Tensor,\n",
    "            infos: List[List[Dict[str, Any]]]) -> th.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Take as input the observations, features (embeddings), \n",
    "            actions and infos, and return the loss induced by them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from typing import Any, Dict, Optional, Type, Union, Tuple\n",
    "from stable_baselines3.common.policies import (\n",
    "    ActorCriticPolicy, BasePolicy, \n",
    "    ActorCriticCnnPolicy)\n",
    "from stable_baselines3.common.type_aliases import GymEnv, Schedule\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.utils import obs_as_tensor, explained_variance\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "    \n",
    "class AuxiliaryPolicy(ActorCriticPolicy):\n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Optional[th.Tensor], th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            entropy of the action distribution, and the features.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        entropy = distribution.entropy()\n",
    "        return values, log_prob, entropy, features\n",
    "\n",
    "class AuxiliaryCnnPolicy(ActorCriticCnnPolicy):\n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Optional[th.Tensor], th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            entropy of the action distribution, and the features.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        entropy = distribution.entropy()\n",
    "        return values, log_prob, entropy, features\n",
    "\n",
    "class AuxiliaryA2C(A2C):\n",
    "    \"\"\"\n",
    "    Advantage Actor Critic (A2C) with the ability to provide an Auxiliary Objective using a Callback\n",
    "    Paper: https://arxiv.org/abs/1602.01783\n",
    "    Code: This implementation borrows code from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    and Stable Baselines (https://github.com/hill-a/stable-baselines)\n",
    "    Introduction to A2C: https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param rms_prop_eps: RMSProp epsilon. It stabilizes square root computation in denominator\n",
    "        of RMSProp update\n",
    "    :param use_rms_prop: Whether to use RMSprop (default) or Adam as optimizer\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    :param auxiliary_objective: The auxiliary objective, whose loss to add to training\n",
    "    :param auxiliary_coef: Auxiliary function coefficient for the loss calculation \n",
    "    \"\"\"\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"AuxiliaryMlpPolicy\": AuxiliaryPolicy,\n",
    "        \"AuxiliaryCnnPolicy\": AuxiliaryCnnPolicy,\n",
    "    }\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: Union[GymEnv, str],\n",
    "            policy: Union[str, Type[AuxiliaryPolicy]],\n",
    "            learning_rate: Union[float, Schedule] = 7e-4,\n",
    "            n_steps: int = 5,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 1.0,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            rms_prop_eps: float = 1e-5,\n",
    "            use_rms_prop: bool = True,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            normalize_advantage: bool = False,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            auxiliary_objective: AuxiliaryObjective = None,\n",
    "            auxiliary_coef: float = 0.5,\n",
    "            ):\n",
    "\n",
    "        self.auxiliary_objective = auxiliary_objective\n",
    "        self.auxiliay_coef = auxiliary_coef\n",
    "\n",
    "        super().__init__(\n",
    "            policy = policy,\n",
    "            env = env,\n",
    "            learning_rate= learning_rate,\n",
    "            n_steps= n_steps,\n",
    "            gamma = gamma,\n",
    "            gae_lambda= gae_lambda,\n",
    "            ent_coef = ent_coef,\n",
    "            vf_coef = vf_coef,\n",
    "            max_grad_norm = max_grad_norm,\n",
    "            rms_prop_eps = rms_prop_eps,\n",
    "            use_rms_prop = use_rms_prop,\n",
    "            use_sde = use_sde,\n",
    "            sde_sample_freq = sde_sample_freq,\n",
    "            normalize_advantage = normalize_advantage,\n",
    "            tensorboard_log = tensorboard_log,\n",
    "            policy_kwargs = policy_kwargs,\n",
    "            verbose = verbose,\n",
    "            seed = seed,\n",
    "            device = device,\n",
    "            _init_setup_model = False\n",
    "        )\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        self.rollout_buffer = AuxiliaryBuffer(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        self.policy = self.policy_class(  # pytype:disable=not-instantiable\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            self.lr_schedule,\n",
    "            use_sde=self.use_sde,\n",
    "            **self.policy_kwargs  # pytype:disable=not-instantiable\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "    def collect_rollouts(\n",
    "            self,\n",
    "            env: VecEnv,\n",
    "            callback: BaseCallback,\n",
    "            rollout_buffer: AuxiliaryBuffer,\n",
    "            n_rollout_steps: int,\n",
    "        ) -> bool:\n",
    "            \"\"\"\n",
    "            Collect experiences using the current policy and fill a ``AuxiliaryBuffer``.\n",
    "            The term rollout here refers to the model-free notion and should not\n",
    "            be used with the concept of rollout used in model-based RL or planning.\n",
    "            :param env: The training environment\n",
    "            :param callback: Callback that will be called at each step\n",
    "                (and at the beginning and end of the rollout)\n",
    "            :param rollout_buffer: Buffer to fill with rollouts\n",
    "            :param n_rollout_steps: Number of experiences to collect per environment\n",
    "            :return: True if function returned with at least `n_rollout_steps`\n",
    "                collected, False if callback terminated rollout prematurely.\n",
    "            \"\"\"\n",
    "            assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "            # Switch to eval mode (this affects batch norm / dropout)\n",
    "            self.policy.set_training_mode(False)\n",
    "\n",
    "            n_steps = 0\n",
    "            rollout_buffer.reset()\n",
    "            # Sample new weights for the state dependent exploration\n",
    "            if self.use_sde:\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "            callback.on_rollout_start()\n",
    "\n",
    "            while n_steps < n_rollout_steps:\n",
    "                if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                    # Sample a new noise matrix\n",
    "                    self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "                with th.no_grad():\n",
    "                    # Convert to pytorch tensor or to TensorDict\n",
    "                    obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                    actions, values, log_probs = self.policy(obs_tensor)\n",
    "                actions = actions.cpu().numpy()\n",
    "\n",
    "                # Rescale and perform action\n",
    "                clipped_actions = actions\n",
    "                # Clip the actions to avoid out of bound error\n",
    "                if isinstance(self.action_space, spaces.Box):\n",
    "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "                new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "                self.num_timesteps += env.num_envs\n",
    "\n",
    "                # Give access to local variables\n",
    "                callback.update_locals(locals())\n",
    "                if callback.on_step() is False:\n",
    "                    return False\n",
    "\n",
    "                self._update_info_buffer(infos)\n",
    "                n_steps += 1\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Reshape in case of discrete action\n",
    "                    actions = actions.reshape(-1, 1)\n",
    "\n",
    "                # Handle timeout by bootstraping with value function\n",
    "                # see GitHub issue #633\n",
    "                for idx, done in enumerate(dones):\n",
    "                    if (\n",
    "                        done\n",
    "                        and infos[idx].get(\"terminal_observation\") is not None\n",
    "                        and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                    ):\n",
    "                        terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                        with th.no_grad():\n",
    "                            terminal_value = self.policy.predict_values(terminal_obs)[0]\n",
    "                        rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "                rollout_buffer.add(self._last_obs, actions, rewards, self._last_episode_starts, values, log_probs, infos)\n",
    "                self._last_obs = new_obs\n",
    "                self._last_episode_starts = dones\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Compute value for the last timestep\n",
    "                values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))\n",
    "\n",
    "            rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "            callback.on_rollout_end()\n",
    "\n",
    "            return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "            \"\"\"\n",
    "            Update policy using the currently gathered\n",
    "            rollout buffer (one gradient step over whole data).\n",
    "            \"\"\"\n",
    "            # Switch to train mode (this affects batch norm / dropout)\n",
    "            self.policy.set_training_mode(True)\n",
    "\n",
    "            # Update optimizer learning rate\n",
    "            self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "            # This will only loop once (get all data in one go)\n",
    "            for rollout_data in self.rollout_buffer.get(batch_size=None):\n",
    "\n",
    "                actions = rollout_data.actions\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = actions.long().flatten()\n",
    "\n",
    "                values, log_prob, entropy, features = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "\n",
    "                # Normalize advantage (not present in the original implementation)\n",
    "                advantages = rollout_data.advantages\n",
    "                if self.normalize_advantage:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy gradient loss\n",
    "                policy_loss = -(advantages * log_prob).mean()\n",
    "\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values)\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                # Auxiliary loss\n",
    "                if self.auxiliary_objective is not None:\n",
    "                    auxiliary_loss = self.auxiliary_objective.calculate_loss(\n",
    "                        obs = rollout_data.observations, features=features,\n",
    "                        actions = rollout_data.actions, returns = rollout_data.returns,\n",
    "                        infos = rollout_data.infos\n",
    "                    )\n",
    "                else:\n",
    "                    auxiliary_loss = None\n",
    "            \n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "                if auxiliary_loss is not None:\n",
    "                    loss = loss + self.auxiliay_coef * auxiliary_loss\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "            self._n_updates += 1\n",
    "            self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "            self.logger.record(\"train/explained_variance\", explained_var)\n",
    "            self.logger.record(\"train/entropy_loss\", entropy_loss.item())\n",
    "            self.logger.record(\"train/policy_loss\", policy_loss.item())\n",
    "            self.logger.record(\"train/value_loss\", value_loss.item())\n",
    "            if auxiliary_loss is not None:\n",
    "                self.logger.record(\"train/auxiliary_loss\", auxiliary_loss.item())\n",
    "            if hasattr(self.policy, \"log_std\"):\n",
    "                self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import numpy as np\n",
    "from typing import Callable, Tuple, Dict\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "def discount_n_step(x: np.ndarray, n_step: int, gamma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Taken from RLlib: https://github.com/ray-project/ray/blob/66650cdadbbc19735d7be4bc613b9c3de30a44da/rllib/evaluation/postprocessing.py#L21\n",
    "    Args:\n",
    "        x: The array of rewards \n",
    "        n_step: The number of steps to look ahead and adjust.\n",
    "        gamma: The discount factor.\n",
    "\n",
    "    Examples:\n",
    "        n-step=3\n",
    "        Trajectory=o0 r0 d0, o1 r1 d1, o2 r2 d2, o3 r3 d3, o4 r4 d4=True o5\n",
    "        gamma=0.9\n",
    "        Returned trajectory:\n",
    "        0: o0 [r0 + 0.9*r1 + 0.9^2*r2 + 0.9^3*r3] d3 o0'=o3\n",
    "        1: o1 [r1 + 0.9*r2 + 0.9^2*r3 + 0.9^3*r4] d4 o1'=o4\n",
    "        2: o2 [r2 + 0.9*r3 + 0.9^2*r4] d4 o1'=o5\n",
    "        3: o3 [r3 + 0.9*r4] d4 o3'=o5\n",
    "        4:\n",
    "    \"\"\"\n",
    "    returns = np.array(x, copy=True, dtype=np.float64)\n",
    "    len_ = returns.shape[0]\n",
    "    # Change rewards in place.\n",
    "    for i in range(len_):\n",
    "        for j in range(1, n_step):\n",
    "            if i + j < len_:\n",
    "                returns[i] += (\n",
    "                    (gamma ** j) * returns[i + j]\n",
    "                )\n",
    "    return returns\n",
    "\n",
    "def discount_n_step_2d(rewards: np.ndarray, n_step: int, gamma: float) -> np.ndarray:\n",
    "    gammas = gamma ** np.arange(n_step)\n",
    "    returns = rewards[:, :n_step] @ gammas\n",
    "    return returns\n",
    "\n",
    "def compute_discounted_threshold(reward: float, n_step: int, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the n_step threshold associated with the reward, assuming that you'd get this exact reward for n_steps.\n",
    "    \n",
    "    Args:\n",
    "        reward: The undiscounted reward to calculate threshold for.\n",
    "        n_step: How many steps we'd get this reward for\n",
    "        gamma: The discount for the n_step\n",
    "    \n",
    "    Returns:\n",
    "        The threshold for the discount\n",
    "    \"\"\"\n",
    "    shape = n_step + 1\n",
    "    rewards = np.array([reward]*shape)\n",
    "    return discount_n_step(rewards, n_step, gamma)[0]\n",
    "\n",
    "\n",
    "def create_minmax_scaler(scale: float, max: float) -> Tuple[Callable, Callable]:\n",
    "    def normalizer(value):\n",
    "        value = value * scale\n",
    "        value = value / max\n",
    "        value = np.clip(value, 1e-10, 1.) # numerical stability\n",
    "        return value\n",
    "\n",
    "    def unnormalizer(value):\n",
    "        value = value * max\n",
    "        value = value / scale\n",
    "        return value\n",
    "\n",
    "    return normalizer, unnormalizer\n",
    "\n",
    "\n",
    "class PredictorTail(nn.Module):\n",
    "    def __init__(self, input_dim: int, net_arch: List[int]):\n",
    "        super(PredictorTail, self).__init__()\n",
    "        modules = []\n",
    "        arch = [input_dim] + net_arch\n",
    "        for idx in range(len(arch) - 1):\n",
    "            modules.append(nn.Linear(arch[idx], arch[idx + 1]))\n",
    "            modules.append(nn.ELU())\n",
    "        self.shared_net = nn.Sequential(*modules)\n",
    "        self.mean_predictor = nn.Sequential(\n",
    "            nn.Linear(arch[-1], 1)\n",
    "        )\n",
    "        self.std_predictor = nn.Sequential(\n",
    "            nn.Linear(arch[-1], 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, data: th.Tensor) -> Normal:\n",
    "        features = self.shared_net(data)\n",
    "        mean, std = self.mean_predictor(features), self.std_predictor(features)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist\n",
    "\n",
    "\n",
    "class DecomposedReturnsObjective(AuxiliaryObjective, nn.Module):\n",
    "    def __init__(self, keys_to_predict: List[str],\n",
    "            scales: Dict[str, float], \n",
    "            maximums: Dict[str, float], \n",
    "            n_step: int, \n",
    "            gamma: float,\n",
    "            feature_dim: int,\n",
    "            action_dim: int,\n",
    "            shared_net_arch: List[int],\n",
    "            predictor_net_arch: List[int],\n",
    "            loss_weights: Dict[str, float],\n",
    "            device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        A unified implementation of an auxiliary loss that predicts decomposed returns.\n",
    "        Args:\n",
    "            keys_to_predict: The keys of the values in the information \n",
    "                dictionary (dict returned by the environment) to predict\n",
    "            scales: The scalers (specified as a dictionary from keys to scaler) \n",
    "                to normalize the values with\n",
    "            maximums: The maximum values of the keys to normalize with \n",
    "                (specified as a dictionary from keys to max values)\n",
    "            n_step: The minimum and maximum n-step to bound the horizon with\n",
    "            gamma: The discount factor to use for the calculation of returns\n",
    "            feature_dim: Feature dim of the features (used to init the shared network)\n",
    "            action_dim: Dimension of the actions (used to init the shared network)\n",
    "            shared_net_arch: The shared network arch (specified as ints of MLP layer size)\n",
    "            predictor_net_arch: The network arch used by each predictor \n",
    "                (specified as ints of MLP laye size)\n",
    "            loss_weights: The weights for each predictor in the final loss calculation\n",
    "                (specified as dict from key to float)\n",
    "            device: The torch to keep the internal model onto\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.keys_to_predict = keys_to_predict\n",
    "        self.scales = scales\n",
    "        self.maximums = maximums\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.loss_weights = loss_weights\n",
    "        self.device = device\n",
    "        self.create_networks(feature_dim = feature_dim, action_dim= action_dim, \n",
    "            shared_net_arch= shared_net_arch, predictor_net_arch= predictor_net_arch)\n",
    "        self.create_normalizers()\n",
    "\n",
    "    def create_normalizers(self):\n",
    "        self.normalizers = {}\n",
    "        self.unnormalizers = {}\n",
    "        for key in self.keys_to_predict:\n",
    "            max = self.maximums[key]\n",
    "            scale = self.scales[key]\n",
    "            max = compute_discounted_threshold(max, self.n_step, self.gamma)\n",
    "            normalizer, unnormalizer = create_minmax_scaler(scale, max)\n",
    "            self.normalizers[key] = normalizer\n",
    "            self.unnormalizers[key] = unnormalizer\n",
    "\n",
    "    def create_networks(self, feature_dim: int, action_dim: int, \n",
    "            shared_net_arch: List[int], predictor_net_arch: List[int]):\n",
    "        arch = [feature_dim + action_dim] + shared_net_arch\n",
    "        predictor_input_dim = shared_net_arch[-1] + action_dim\n",
    "        modules = []\n",
    "        for idx in range(len(arch) - 1):\n",
    "            modules.append(nn.Linear(arch[idx], arch[idx + 1]))\n",
    "            modules.append(nn.ELU())\n",
    "        self.shared_net = nn.Sequential(*modules)\n",
    "        self.shared_net = self.shared_net.to(device = self.device)\n",
    "        for key in self.keys_to_predict:\n",
    "            tail = PredictorTail(input_dim= predictor_input_dim, net_arch=predictor_net_arch)\n",
    "            tail = tail.to(device = self.device)\n",
    "            setattr(self, f\"{key}_tail\", tail)\n",
    "        \n",
    "    def forward(self, features: th.Tensor, actions: th.Tensor) -> Dict[str, Normal]:\n",
    "        output = {}\n",
    "        shared_features = self.shared_net(th.cat((features, actions), dim=1))\n",
    "        shared_features = th.cat((shared_features, actions), dim=1)\n",
    "        for key in self.keys_to_predict:\n",
    "            tail = getattr(self, f\"{key}_tail\")\n",
    "            pred = tail(shared_features)\n",
    "            output[key] = pred\n",
    "        return output\n",
    "\n",
    "    def calculate_loss(self, obs: th.Tensor, \n",
    "            features: th.Tensor, \n",
    "            actions: th.Tensor, \n",
    "            returns: th.Tensor, \n",
    "            infos: List[List[Dict[str, Any]]]) -> th.Tensor:\n",
    "        mask = th.from_numpy(np.array([len(info_samples) >= self.n_step for info_samples in infos]))\n",
    "        masked_features = features[mask]\n",
    "        masked_actions = actions[mask]\n",
    "        pred = self.forward(features = masked_features, actions = masked_actions)\n",
    "        loss = None\n",
    "\n",
    "        for key in self.keys_to_predict:\n",
    "            rewards = []\n",
    "            for i, info_samples in enumerate(infos):\n",
    "                if not mask[i].item():\n",
    "                    continue\n",
    "                rewards.append([info.get(key) for info in info_samples[:self.n_step]])\n",
    "            facet_returns = discount_n_step_2d(rewards = np.array(rewards), \n",
    "                n_step=self.n_step, gamma=self.gamma)\n",
    "            target = th.from_numpy(self.normalizers[key](facet_returns))\n",
    "            target = target.to(device= self.device)\n",
    "            target = target.reshape(-1, 1)\n",
    "            facet_loss = -th.mean(pred[key].log_prob(target))\n",
    "            if loss is None:\n",
    "                loss = self.loss_weights[key] * facet_loss\n",
    "            else:\n",
    "                loss += self.loss_weights[key] * facet_loss\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'cc-rl-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import cc_rl\n",
    "\n",
    "class MlpExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, output_dim: int = 256):\n",
    "        super().__init__(observation_space, output_dim)\n",
    "        n_inputs = np.prod(observation_space.shape)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_inputs, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.mlp(observations)\n",
    "\n",
    "KEYS_TO_PREDICT = [\"throughput\", \"latency\", \"loss\"]\n",
    "SCALES = {\n",
    "    \"throughput\": 1/10,\n",
    "    \"latency\": -(1/1000),\n",
    "    \"loss\": -(1/2000)\n",
    "}\n",
    "MAXIMUMS = {\n",
    "    \"throughput\": 500,\n",
    "    \"latency\": 5,\n",
    "    \"loss\": 1.0 \n",
    "}\n",
    "LOSS_WEIGHTS = {\n",
    "    \"throughput\": 0.3,\n",
    "    \"latency\": 0.3,\n",
    "    \"loss\": 0.4,\n",
    "}\n",
    "N_STEP = 5\n",
    "GAMMA = 0.95\n",
    "FEATURE_DIM = 256\n",
    "ACTION_DIM = 1\n",
    "SHARED_NET_ARCH = [128, 128]\n",
    "PREDICTOR_NET_ARCH = [64]\n",
    "\n",
    "\n",
    "objective = DecomposedReturnsObjective(\n",
    "    keys_to_predict= KEYS_TO_PREDICT,\n",
    "    scales = SCALES,\n",
    "    maximums= MAXIMUMS,\n",
    "    n_step = N_STEP,\n",
    "    gamma = GAMMA,\n",
    "    feature_dim= FEATURE_DIM,\n",
    "    action_dim= ACTION_DIM,\n",
    "    shared_net_arch= SHARED_NET_ARCH,\n",
    "    predictor_net_arch= PREDICTOR_NET_ARCH,\n",
    "    loss_weights= LOSS_WEIGHTS,\n",
    "    device= \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "model = AuxiliaryA2C(env = 'cc-rl-v0', policy=\"AuxiliaryMlpPolicy\", \n",
    "    auxiliary_objective=objective, policy_kwargs=dict(features_extractor_class = MlpExtractor), n_steps=20, verbose=1, auxiliary_coef=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 100       |\n",
      "|    ep_rew_mean        | 102       |\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 0.575     |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | -0.000474 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.74      |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 8.81      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AuxiliaryA2C at 0x7f4bbe2581c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('puffer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ca6f4f18def67e45566ff1c421a37c5f97dd8229b869c68c597549ec78f42b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
