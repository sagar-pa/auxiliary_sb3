{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Union, Dict, Any, Optional\n",
    "import torch as th\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class AuxiliaryBufferSamples(NamedTuple):\n",
    "    observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    old_values: th.Tensor\n",
    "    old_log_prob: th.Tensor\n",
    "    advantages: th.Tensor\n",
    "    returns: th.Tensor\n",
    "    infos: List[List[Dict[str, Any]]]\n",
    "\n",
    "class AuxiliaryBuffer(RolloutBuffer):\n",
    "    \"\"\"\n",
    "    Rollout buffer used in on-policy algorithms like A2C/PPO, \n",
    "        with an added tracking of information dictionaries.\n",
    "    It corresponds to ``buffer_size`` transitions collected\n",
    "    using the current policy.\n",
    "    This experience will be discarded after the policy update.\n",
    "    In order to use PPO objective, we also store the current value of each state\n",
    "    and the log probability of each taken action.\n",
    "    The term rollout here refers to the model-free notion and should not\n",
    "    be used with the concept of rollout used in model-based RL or planning.\n",
    "    Hence, it is only involved in policy and value function training but not action selection.\n",
    "    :param buffer_size: Max number of element in the buffer\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param device: PyTorch device\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param gamma: Discount factor\n",
    "    :param n_envs: Number of parallel environments\n",
    "    \"\"\"\n",
    "    def __init__(self,         \n",
    "            buffer_size: int,\n",
    "            observation_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            gae_lambda: float = 1,\n",
    "            gamma: float = 0.99,\n",
    "            n_envs: int = 1):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, \n",
    "            gae_lambda=gae_lambda, gamma=gamma, n_envs=n_envs)\n",
    "        \n",
    "        lengths = [buffer_size] * n_envs\n",
    "        self.info_bins = np.cumsum(np.array(lengths))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.infos = [[None] * self.buffer_size for __ in range(self.n_envs)]\n",
    "        super().reset()\n",
    "\n",
    "    def add(self,\n",
    "            obs: np.ndarray,\n",
    "            action: np.ndarray,\n",
    "            reward: np.ndarray,\n",
    "            episode_start: np.ndarray,\n",
    "            value: th.Tensor,\n",
    "            log_prob: th.Tensor,\n",
    "            infos: List[Dict[str, Any]], \n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        :param obs: Observation\n",
    "        :param action: Action\n",
    "        :param reward:\n",
    "        :param episode_start: Start of episode signal.\n",
    "        :param value: estimated value of the current state\n",
    "            following the current policy.\n",
    "        :param log_prob: log probability of the action\n",
    "            following the current policy.\n",
    "        :param infos: the information dictionaries returned by the environment\n",
    "        \"\"\"\n",
    "        for i, info in enumerate(infos):\n",
    "            self.infos[i][self.pos] = deepcopy(info)\n",
    "        super().add(obs, action, reward, episode_start, value, log_prob)\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> AuxiliaryBufferSamples:\n",
    "        rollout_samples = super()._get_samples(batch_inds, env)\n",
    "        infos = []\n",
    "        env_indices = np.digitize(batch_inds, bins=self.info_bins, right=False)\n",
    "        last_env_indices = np.clip(env_indices - 1, 0, self.info_bins.shape[0])\n",
    "        step_indices = batch_inds - self.info_bins[last_env_indices]\n",
    "        for ep_idx, step_idx in zip(env_indices, step_indices):\n",
    "            infos.append(self.infos[ep_idx][step_idx:])\n",
    "        return AuxiliaryBufferSamples(*rollout_samples, infos=infos)\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/anaconda3/envs/puffer/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "obs_space = spaces.Box(low=np.zeros((3,), dtype=np.float64), high = np.ones((3,), dtype=np.float64))\n",
    "action_space = spaces.Box(low=np.zeros(1, dtype=np.float64), high=np.ones(1, dtype=np.float64))\n",
    "\n",
    "buffer = AuxiliaryBuffer(buffer_size= 10, observation_space=obs_space, action_space = action_space, device=\"cpu\", n_envs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_env = 3\n",
    "for i in range(10):\n",
    "    obs = np.random.randn(n_env, 3,)\n",
    "    act = np.random.randn(n_env, 1,)\n",
    "    reward = np.random.randn(n_env, )\n",
    "    dones = np.random.randn(n_env, ) > 0.5\n",
    "    value = th.randn(n_env, )\n",
    "    log_prob = th.randn(n_env, )\n",
    "    infos = [{\"i\": i, \"j\": j} for j in range(n_env)]\n",
    "    buffer.add(obs, act, reward, dones, value, log_prob, infos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = buffer.get(batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in batch:\n",
    "    print(data.infos)\n",
    "buffer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Dict, Any\n",
    "import torch as th\n",
    "\n",
    "class AuxiliaryObjective(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate_loss(self, \n",
    "            obs: th.Tensor,\n",
    "            features: th.Tensor,\n",
    "            actions: th.Tensor,\n",
    "            returns: th.Tensor,\n",
    "            infos: List[List[Dict[str, Any]]]) -> th.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Take as input the observations, features (embeddings), \n",
    "            actions and infos, and return the loss induced by them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from typing import Any, Dict, Optional, Type, Union, Tuple\n",
    "from stable_baselines3.common.policies import (\n",
    "    ActorCriticPolicy, BasePolicy, \n",
    "    ActorCriticCnnPolicy, MultiInputActorCriticPolicy)\n",
    "from stable_baselines3.common.type_aliases import GymEnv, Schedule\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.utils import obs_as_tensor, explained_variance\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "    \n",
    "class AuxiliaryPolicy(ActorCriticPolicy):\n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Optional[th.Tensor], th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            entropy of the action distribution, and the features.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        entropy = distribution.entropy()\n",
    "        return values, log_prob, entropy, features\n",
    "\n",
    "class AuxiliaryCnnPolicy(ActorCriticCnnPolicy):\n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Optional[th.Tensor], th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            entropy of the action distribution, and the features.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        entropy = distribution.entropy()\n",
    "        return values, log_prob, entropy, features\n",
    "\n",
    "class AuxiliaryA2C(A2C):\n",
    "    \"\"\"\n",
    "    Advantage Actor Critic (A2C) with the ability to provide an Auxiliary Objective using a Callback\n",
    "    Paper: https://arxiv.org/abs/1602.01783\n",
    "    Code: This implementation borrows code from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
    "    and Stable Baselines (https://github.com/hill-a/stable-baselines)\n",
    "    Introduction to A2C: https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param rms_prop_eps: RMSProp epsilon. It stabilizes square root computation in denominator\n",
    "        of RMSProp update\n",
    "    :param use_rms_prop: Whether to use RMSprop (default) or Adam as optimizer\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    :param auxiliary_objective: The auxiliary objective, whose loss to add to training\n",
    "    :param auxiliary_coef: Auxiliary function coefficient for the loss calculation \n",
    "    \"\"\"\n",
    "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
    "        \"AuxiliaryMlpPolicy\": AuxiliaryPolicy,\n",
    "        \"AuxiliaryCnnPolicy\": AuxiliaryCnnPolicy,\n",
    "    }\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: Union[GymEnv, str],\n",
    "            policy: Union[str, Type[AuxiliaryPolicy]],\n",
    "            learning_rate: Union[float, Schedule] = 7e-4,\n",
    "            n_steps: int = 5,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 1.0,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            rms_prop_eps: float = 1e-5,\n",
    "            use_rms_prop: bool = True,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            normalize_advantage: bool = False,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            auxiliary_objective: AuxiliaryObjective = None,\n",
    "            auxiliary_coef: float = 0.5,\n",
    "            ):\n",
    "\n",
    "        self.auxiliary_objective = auxiliary_objective\n",
    "        self.auxiliay_coef = auxiliary_coef\n",
    "\n",
    "        super().__init__(\n",
    "            policy = policy,\n",
    "            env = env,\n",
    "            learning_rate= learning_rate,\n",
    "            n_steps= n_steps,\n",
    "            gamma = gamma,\n",
    "            gae_lambda= gae_lambda,\n",
    "            ent_coef = ent_coef,\n",
    "            vf_coef = vf_coef,\n",
    "            max_grad_norm = max_grad_norm,\n",
    "            rms_prop_eps = rms_prop_eps,\n",
    "            use_rms_prop = use_rms_prop,\n",
    "            use_sde = use_sde,\n",
    "            sde_sample_freq = sde_sample_freq,\n",
    "            normalize_advantage = normalize_advantage,\n",
    "            tensorboard_log = tensorboard_log,\n",
    "            policy_kwargs = policy_kwargs,\n",
    "            verbose = verbose,\n",
    "            seed = seed,\n",
    "            device = device,\n",
    "            _init_setup_model = False\n",
    "        )\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        self.rollout_buffer = AuxiliaryBuffer(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        self.policy = self.policy_class(  # pytype:disable=not-instantiable\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            self.lr_schedule,\n",
    "            use_sde=self.use_sde,\n",
    "            **self.policy_kwargs  # pytype:disable=not-instantiable\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "    def collect_rollouts(\n",
    "            self,\n",
    "            env: VecEnv,\n",
    "            callback: BaseCallback,\n",
    "            rollout_buffer: AuxiliaryBuffer,\n",
    "            n_rollout_steps: int,\n",
    "        ) -> bool:\n",
    "            \"\"\"\n",
    "            Collect experiences using the current policy and fill a ``AuxiliaryBuffer``.\n",
    "            The term rollout here refers to the model-free notion and should not\n",
    "            be used with the concept of rollout used in model-based RL or planning.\n",
    "            :param env: The training environment\n",
    "            :param callback: Callback that will be called at each step\n",
    "                (and at the beginning and end of the rollout)\n",
    "            :param rollout_buffer: Buffer to fill with rollouts\n",
    "            :param n_rollout_steps: Number of experiences to collect per environment\n",
    "            :return: True if function returned with at least `n_rollout_steps`\n",
    "                collected, False if callback terminated rollout prematurely.\n",
    "            \"\"\"\n",
    "            assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "            # Switch to eval mode (this affects batch norm / dropout)\n",
    "            self.policy.set_training_mode(False)\n",
    "\n",
    "            n_steps = 0\n",
    "            rollout_buffer.reset()\n",
    "            # Sample new weights for the state dependent exploration\n",
    "            if self.use_sde:\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "            callback.on_rollout_start()\n",
    "\n",
    "            while n_steps < n_rollout_steps:\n",
    "                if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                    # Sample a new noise matrix\n",
    "                    self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "                with th.no_grad():\n",
    "                    # Convert to pytorch tensor or to TensorDict\n",
    "                    obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                    actions, values, log_probs = self.policy(obs_tensor)\n",
    "                actions = actions.cpu().numpy()\n",
    "\n",
    "                # Rescale and perform action\n",
    "                clipped_actions = actions\n",
    "                # Clip the actions to avoid out of bound error\n",
    "                if isinstance(self.action_space, spaces.Box):\n",
    "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "                new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "                self.num_timesteps += env.num_envs\n",
    "\n",
    "                # Give access to local variables\n",
    "                callback.update_locals(locals())\n",
    "                if callback.on_step() is False:\n",
    "                    return False\n",
    "\n",
    "                self._update_info_buffer(infos)\n",
    "                n_steps += 1\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Reshape in case of discrete action\n",
    "                    actions = actions.reshape(-1, 1)\n",
    "\n",
    "                # Handle timeout by bootstraping with value function\n",
    "                # see GitHub issue #633\n",
    "                for idx, done in enumerate(dones):\n",
    "                    if (\n",
    "                        done\n",
    "                        and infos[idx].get(\"terminal_observation\") is not None\n",
    "                        and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                    ):\n",
    "                        terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                        with th.no_grad():\n",
    "                            terminal_value = self.policy.predict_values(terminal_obs)[0]\n",
    "                        rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "                rollout_buffer.add(self._last_obs, actions, rewards, self._last_episode_starts, values, log_probs, infos)\n",
    "                self._last_obs = new_obs\n",
    "                self._last_episode_starts = dones\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Compute value for the last timestep\n",
    "                values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))\n",
    "\n",
    "            rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "            callback.on_rollout_end()\n",
    "\n",
    "            return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "            \"\"\"\n",
    "            Update policy using the currently gathered\n",
    "            rollout buffer (one gradient step over whole data).\n",
    "            \"\"\"\n",
    "            # Switch to train mode (this affects batch norm / dropout)\n",
    "            self.policy.set_training_mode(True)\n",
    "\n",
    "            # Update optimizer learning rate\n",
    "            self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "            # This will only loop once (get all data in one go)\n",
    "            for rollout_data in self.rollout_buffer.get(batch_size=None):\n",
    "\n",
    "                actions = rollout_data.actions\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = actions.long().flatten()\n",
    "\n",
    "                values, log_prob, entropy, features = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "\n",
    "                # Normalize advantage (not present in the original implementation)\n",
    "                advantages = rollout_data.advantages\n",
    "                if self.normalize_advantage:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy gradient loss\n",
    "                policy_loss = -(advantages * log_prob).mean()\n",
    "\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values)\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                # Auxiliary loss\n",
    "                if self.auxiliary_objective is not None:\n",
    "                    auxiliary_loss = self.auxiliary_objective.calculate_loss(\n",
    "                        obs = rollout_data.observations, features=features,\n",
    "                        actions = rollout_data.actions, returns = rollout_data.returns,\n",
    "                        infos = rollout_data.infos\n",
    "                    )\n",
    "                else:\n",
    "                    auxiliary_loss = None\n",
    "            \n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "                if auxiliary_loss is not None:\n",
    "                    loss = loss + self.auxiliay_coef * auxiliary_loss\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "            self._n_updates += 1\n",
    "            self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "            self.logger.record(\"train/explained_variance\", explained_var)\n",
    "            self.logger.record(\"train/entropy_loss\", entropy_loss.item())\n",
    "            self.logger.record(\"train/policy_loss\", policy_loss.item())\n",
    "            self.logger.record(\"train/value_loss\", value_loss.item())\n",
    "            if auxiliary_loss is not None:\n",
    "                self.logger.record(\"train/auxiliary_loss\", auxiliary_loss.item())\n",
    "            if hasattr(self.policy, \"log_std\"):\n",
    "                self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class ReturnObjective(AuxiliaryObjective):\n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        super().__init__()\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "\n",
    "    def calculate_loss(self, \n",
    "            obs: th.Tensor,\n",
    "            features: th.Tensor,\n",
    "            actions: th.Tensor,\n",
    "            returns: th.Tensor,\n",
    "            infos: List[List[Dict[str, Any]]]) -> th.Tensor:\n",
    "        predicted_returns = self.model(features)\n",
    "        loss = F.mse_loss(predicted_returns, returns.unsqueeze(1))\n",
    "        return loss\n",
    "        \n",
    "class MlpExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, output_dim: int = 256):\n",
    "        super().__init__(observation_space, output_dim)\n",
    "        n_inputs = np.prod(observation_space.shape)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_inputs, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.mlp(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'LunarLander-v2'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "objective = ReturnObjective()\n",
    "\n",
    "model = AuxiliaryA2C(\"LunarLander-v2\", policy=\"AuxiliaryCnnPolicy\", \n",
    "    auxiliary_objective=objective, policy_kwargs=dict(features_extractor_class = MlpExtractor), n_steps=20, verbose=1, auxiliary_coef=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 105       |\n",
      "|    ep_rew_mean        | -269      |\n",
      "| time/                 |           |\n",
      "|    fps                | 525       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 4.81e+03  |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | -0.000231 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -68.3     |\n",
      "|    value_loss         | 3.43e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 109      |\n",
      "|    ep_rew_mean        | -239     |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 284      |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.00832  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -3.22    |\n",
      "|    value_loss         | 10       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 114       |\n",
      "|    ep_rew_mean        | -242      |\n",
      "| time/                 |           |\n",
      "|    fps                | 532       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 3.7e+03   |\n",
      "|    entropy_loss       | -1.01     |\n",
      "|    explained_variance | -6.96e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -36       |\n",
      "|    value_loss         | 2.03e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 119       |\n",
      "|    ep_rew_mean        | -233      |\n",
      "| time/                 |           |\n",
      "|    fps                | 529       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 115       |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | -0.000174 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 30.4      |\n",
      "|    value_loss         | 679       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 127       |\n",
      "|    ep_rew_mean        | -223      |\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 384       |\n",
      "|    entropy_loss       | -1.04     |\n",
      "|    explained_variance | -0.000326 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 0.755     |\n",
      "|    value_loss         | 20.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 133       |\n",
      "|    ep_rew_mean        | -205      |\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 2.83e+03  |\n",
      "|    entropy_loss       | -1.2      |\n",
      "|    explained_variance | -0.000212 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -26.4     |\n",
      "|    value_loss         | 1.45e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 141       |\n",
      "|    ep_rew_mean        | -189      |\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    auxiliary_loss     | 652       |\n",
      "|    entropy_loss       | -0.969    |\n",
      "|    explained_variance | -1.65e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -4.5      |\n",
      "|    value_loss         | 35.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 150      |\n",
      "|    ep_rew_mean        | -170     |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 188      |\n",
      "|    entropy_loss       | -0.879   |\n",
      "|    explained_variance | 0.00541  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 4.61     |\n",
      "|    value_loss         | 42.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 164      |\n",
      "|    ep_rew_mean        | -164     |\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 28.1     |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.249    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.05     |\n",
      "|    value_loss         | 8.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 182      |\n",
      "|    ep_rew_mean        | -164     |\n",
      "| time/                 |          |\n",
      "|    fps                | 472      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 6.84     |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.11     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 5.7      |\n",
      "|    value_loss         | 55.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 207      |\n",
      "|    ep_rew_mean        | -158     |\n",
      "| time/                 |          |\n",
      "|    fps                | 458      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 492      |\n",
      "|    entropy_loss       | -0.971   |\n",
      "|    explained_variance | -0.0907  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -30.7    |\n",
      "|    value_loss         | 406      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 226      |\n",
      "|    ep_rew_mean        | -158     |\n",
      "| time/                 |          |\n",
      "|    fps                | 445      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 88.5     |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.23     |\n",
      "|    value_loss         | 4.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 235      |\n",
      "|    ep_rew_mean        | -153     |\n",
      "| time/                 |          |\n",
      "|    fps                | 441      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 299      |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.272    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -5       |\n",
      "|    value_loss         | 19.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 253      |\n",
      "|    ep_rew_mean        | -149     |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 48.6     |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.566    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.176   |\n",
      "|    value_loss         | 4.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 275      |\n",
      "|    ep_rew_mean        | -142     |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 14.1     |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | -0.0903  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -4.47    |\n",
      "|    value_loss         | 23.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 293      |\n",
      "|    ep_rew_mean        | -136     |\n",
      "| time/                 |          |\n",
      "|    fps                | 432      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 77       |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | -0.287   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -6.86    |\n",
      "|    value_loss         | 43.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 308      |\n",
      "|    ep_rew_mean        | -138     |\n",
      "| time/                 |          |\n",
      "|    fps                | 429      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 89.2     |\n",
      "|    entropy_loss       | -0.834   |\n",
      "|    explained_variance | 0.421    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -2.05    |\n",
      "|    value_loss         | 8.17     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | -131     |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 8.16     |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.517    |\n",
      "|    value_loss         | 1.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 345      |\n",
      "|    ep_rew_mean        | -127     |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 716      |\n",
      "|    entropy_loss       | -0.686   |\n",
      "|    explained_variance | 0.435    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 6.73     |\n",
      "|    value_loss         | 246      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 362      |\n",
      "|    ep_rew_mean        | -119     |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 10.1     |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.909    |\n",
      "|    value_loss         | 1.37     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 380      |\n",
      "|    ep_rew_mean        | -107     |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 64.5     |\n",
      "|    entropy_loss       | -0.175   |\n",
      "|    explained_variance | -0.00281 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.914    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 395      |\n",
      "|    ep_rew_mean        | -88.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 277      |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.136    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -1.62    |\n",
      "|    value_loss         | 8.31     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | -78.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 26.3     |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -1.92    |\n",
      "|    value_loss         | 5.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 422      |\n",
      "|    ep_rew_mean        | -55      |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 118      |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.266   |\n",
      "|    value_loss         | 0.755    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 437      |\n",
      "|    ep_rew_mean        | -34.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 183      |\n",
      "|    entropy_loss       | -0.852   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 1.15     |\n",
      "|    value_loss         | 2.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | -22.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 422      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 123      |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 167      |\n",
      "|    entropy_loss       | -0.88    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.741    |\n",
      "|    value_loss         | 0.798    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 460      |\n",
      "|    ep_rew_mean        | -3.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 423      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 388      |\n",
      "|    entropy_loss       | -0.566   |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 1.66     |\n",
      "|    value_loss         | 4.46     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 468      |\n",
      "|    ep_rew_mean        | 9.35     |\n",
      "| time/                 |          |\n",
      "|    fps                | 424      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 6.8e+03  |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -23.2    |\n",
      "|    value_loss         | 6.26e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 473      |\n",
      "|    ep_rew_mean        | 20.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 425      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 136      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 6.96e+03 |\n",
      "|    entropy_loss       | -0.224   |\n",
      "|    explained_variance | 0.362    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -13.3    |\n",
      "|    value_loss         | 5.73e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 481      |\n",
      "|    ep_rew_mean        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 426      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 1.06e+03 |\n",
      "|    entropy_loss       | -0.654   |\n",
      "|    explained_variance | 0.509    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 5.16     |\n",
      "|    value_loss         | 133      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 489      |\n",
      "|    ep_rew_mean        | 39.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 137      |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 1.4      |\n",
      "|    value_loss         | 2.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 493      |\n",
      "|    ep_rew_mean        | 44.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 430      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 579      |\n",
      "|    entropy_loss       | -0.759   |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -2.07    |\n",
      "|    value_loss         | 4.61     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 486      |\n",
      "|    ep_rew_mean        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 257      |\n",
      "|    entropy_loss       | -0.771   |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 1.06     |\n",
      "|    value_loss         | 4.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 466      |\n",
      "|    ep_rew_mean        | 54       |\n",
      "| time/                 |          |\n",
      "|    fps                | 431      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 157      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 96       |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 1.11     |\n",
      "|    value_loss         | 5.51     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 436      |\n",
      "|    ep_rew_mean        | 54.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 161      |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 524      |\n",
      "|    entropy_loss       | -0.71    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.41    |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 374      |\n",
      "|    ep_rew_mean        | 62.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 165      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 33.1     |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.916   |\n",
      "|    value_loss         | 3.69     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 332      |\n",
      "|    ep_rew_mean        | 58       |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 379      |\n",
      "|    entropy_loss       | -0.707   |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 42.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 619      |\n",
      "|    entropy_loss       | -0.9     |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.671    |\n",
      "|    value_loss         | 3.39     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 40.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 3.06e+03 |\n",
      "|    entropy_loss       | -0.112   |\n",
      "|    explained_variance | -0.0963  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0641   |\n",
      "|    value_loss         | 2.5e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 34.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 437      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 182      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 953      |\n",
      "|    entropy_loss       | -0.558   |\n",
      "|    explained_variance | 0.102    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 28.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 8.38     |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | -0.247   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 4.07     |\n",
      "|    value_loss         | 19.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 333      |\n",
      "|    ep_rew_mean        | 21       |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 363      |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.279    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.0858   |\n",
      "|    value_loss         | 0.961    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 347      |\n",
      "|    ep_rew_mean        | 19.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 433      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 198      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 224      |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.501    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -1.18    |\n",
      "|    value_loss         | 4.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 360      |\n",
      "|    ep_rew_mean        | 14.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 428      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 205      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 241      |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | 0.386    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.933    |\n",
      "|    value_loss         | 1.57     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 375      |\n",
      "|    ep_rew_mean        | 11       |\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 213      |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 14       |\n",
      "|    entropy_loss       | -0.74    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.362    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 221      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 27.9     |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.236    |\n",
      "|    value_loss         | 0.438    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 393      |\n",
      "|    ep_rew_mean        | 5.84     |\n",
      "| time/                 |          |\n",
      "|    fps                | 410      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 672      |\n",
      "|    entropy_loss       | -0.746   |\n",
      "|    explained_variance | 0.241    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -2.07    |\n",
      "|    value_loss         | 8.35     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 407      |\n",
      "|    ep_rew_mean        | 1.65     |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 236      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 117      |\n",
      "|    entropy_loss       | -0.793   |\n",
      "|    explained_variance | 0.646    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.699    |\n",
      "|    value_loss         | 2.46     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 422      |\n",
      "|    ep_rew_mean        | 1.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 402      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 243      |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 114      |\n",
      "|    entropy_loss       | -0.826   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.147    |\n",
      "|    value_loss         | 0.407    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 440      |\n",
      "|    ep_rew_mean        | -1.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 397      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 7.78     |\n",
      "|    entropy_loss       | -0.648   |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.609    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 454      |\n",
      "|    ep_rew_mean        | -0.458   |\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 260      |\n",
      "|    total_timesteps    | 102000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 1.19e+03 |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0.556    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 2.7      |\n",
      "|    value_loss         | 30.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 465      |\n",
      "|    ep_rew_mean        | 0.597    |\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 266      |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 8.99     |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.474    |\n",
      "|    value_loss         | 0.991    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 475      |\n",
      "|    ep_rew_mean        | 4.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 273      |\n",
      "|    total_timesteps    | 106000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 6.25e+03 |\n",
      "|    entropy_loss       | -0.204   |\n",
      "|    explained_variance | 0.207    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -19      |\n",
      "|    value_loss         | 6.19e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 483      |\n",
      "|    ep_rew_mean        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 385      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 279      |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 3.91e+03 |\n",
      "|    entropy_loss       | -0.199   |\n",
      "|    explained_variance | -0.522   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 4.49     |\n",
      "|    value_loss         | 1.5e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 496      |\n",
      "|    ep_rew_mean        | 13.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 287      |\n",
      "|    total_timesteps    | 110000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 15.5     |\n",
      "|    entropy_loss       | -0.702   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -2.68    |\n",
      "|    value_loss         | 4.89     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 505      |\n",
      "|    ep_rew_mean        | 6.81     |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 294      |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 101      |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.332    |\n",
      "|    value_loss         | 1.24     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 512      |\n",
      "|    ep_rew_mean        | 3.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 301      |\n",
      "|    total_timesteps    | 114000   |\n",
      "| train/                |          |\n",
      "|    auxiliary_loss     | 705      |\n",
      "|    entropy_loss       | -0.589   |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -1.77    |\n",
      "|    value_loss         | 5.29     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39m1e6\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m: A2CSelf,\n\u001b[1;32m    191\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m A2CSelf:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    204\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    205\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    206\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    207\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    208\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    209\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    210\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    211\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    212\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    213\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    214\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 210\u001b[0m, in \u001b[0;36mAuxiliaryA2C.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    208\u001b[0m     \u001b[39m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     obs_tensor \u001b[39m=\u001b[39m obs_as_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 210\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor)\n\u001b[1;32m    211\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    213\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/stable_baselines3/common/policies.py:587\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m    586\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs)\n\u001b[0;32m--> 587\u001b[0m latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor(features)\n\u001b[1;32m    588\u001b[0m \u001b[39m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[1;32m    589\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/stable_baselines3/common/torch_layers.py:230\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m:return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m shared_latent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared_net(features)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(shared_latent), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(shared_latent)\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/puffer/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(1e6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('puffer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ca6f4f18def67e45566ff1c421a37c5f97dd8229b869c68c597549ec78f42b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
